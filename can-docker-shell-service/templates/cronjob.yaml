apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ include "service.name" . }}
  labels:
    app.kubernetes.io/name: {{ include "service.name" . }}
    helm.sh/chart: {{ include "service.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    run: {{ include "service.name" . }}
spec:
  schedule: "*/1 * * * *"
  # defines that cron won't start another pod when current workload is still running
  concurrencyPolicy: Forbid
  # Keep only X (one) of the pod's history
  # Set to parallelism value if you want to keep the whole job batch history
  # History is cleaned from newest to oldest (oldest history remains)
  #   Manual cleaning of pods is recommended after you have viewed and studied failure logs,
  #   so you create space for new history
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  # Set to parallelism value * cronjob schedule in seconds
  startingDeadlineSeconds: 120
  jobTemplate:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "service.name" . }}
        helm.sh/chart: {{ include "service.chart" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/managed-by: {{ .Release.Service }}
        run: {{ include "service.name" . }}
    spec:
      # Setting backoffLimit to very high integer means that for every Failed pod a new pod will be started.
      # Can-shell-service exits with status 1 and thanks to the high backoffLimit the new pods will be prepared
      # endlessly for every finished (=Failed) pod.
      # Disadvantage is that the Failed pods are not garbage collected by K8s and must be clean-up manually.
      backoffLimit: 100000
      # Amount of seconds after which Job should be killed
      # Killing a job goes through it's standard terminating procedure, it's not a "force" kill
      # activeDeadlineSeconds: {{ .Values.jobRecyclePeriod }}
      # ^ No longer used, we recycle manually using CI pipeline

      # how many pods start at the same job batch
      parallelism: {{ .Values.replicaCount }}
      # how many pods must be in Completed state to evaluate a Job as completed.
      # Can-shell-service is exit with status 1 and done jobs are set as Failed.
      # so this setting quarantiee that the Job will never be set as Completed
      # and new pods will still be re-created.
      completions: {{ .Values.replicaCount }}
      template:
        metadata:
          annotations:
            prometheus.io/port: "{{ .Values.service.health_port }}"
            prometheus.io/scrape: "true"
            prometheus.io/path: "/actuator/prometheus"
        spec:
          # defines that PODs will be scheduled on NODEs that have label lifecycle=Ec2Spot (I am using affinity since nodeSelector will be deprecated soon)
          # affinity:
          #   nodeAffinity:
          #     requiredDuringSchedulingIgnoredDuringExecution:
          #       nodeSelectorTerms:
          #       - matchExpressions:
          #         - key: hackathon
          #           operator: Exists
          serviceAccountName: {{ .Release.Name }}
          containers:
          - name: {{ .Chart.Name }}
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            # Doesn't apply here because shell-service isn't exposed as any service type
            # lifecycle:
            #   preStop:
            #     exec:
            #       command: [
            #         "sh", "-c",
            #         # Introduce a delay to the shutdown sequence to wait for the
            #         # pod eviction event to propagate. Then, gracefully shutdown.
            #         "sleep 5",
            #       ]
            env:
            {{- $dot := . }}
            {{- range .Values.env.secrets }}
            - name: {{ tpl (index . "name") $dot }}
              valueFrom:
                secretKeyRef:
                  name: {{ tpl (index . "refname") $dot }}
                  key: {{ tpl (index . "refkey") $dot }}
            {{- end }}
            {{- $dot := . }}
            {{- range .Values.env.variables }}
            - name: {{ tpl (index . "name") $dot }}
              value: "{{ tpl (index . "value") $dot }}"
            {{- end }}
            {{- $dot := . }}
            #mapping of temporary volume to the POD (this time to /cache mount point)
            volumeMounts:
            {{- $dot := . }}
            {{- range .Values.volumeMounts }}
            - mountPath: {{ tpl (index . "mountPath") $dot }}
              name: {{ tpl (index . "name") $dot  }}
              {{- if hasKey (index .) "subPath" }}
              subPath: {{ tpl (index . "subPath") $dot }}
              {{- end }}
            {{- end }}
            # spec.jobTemplate.spec.template.spec.containers[0].ports[0].containerPort
            ports:
              - name: health
                containerPort: {{ .Values.service.health_port }}
                protocol: TCP
            livenessProbe:
              httpGet:
                path: /actuator/health/liveness
                port: health
              initialDelaySeconds: 120
              periodSeconds: 30
            readinessProbe:
              httpGet:
                path: /actuator/health/readiness
                port: health
              initialDelaySeconds: 60
              periodSeconds: 30
            resources:
              {{- toYaml .Values.resources | nindent 16 }}
          priorityClassName: {{ .Values.priorityClassName }}
          # Give the pod X seconds to handle SIGTERM signal, otherwise SIGKILL is sent
          terminationGracePeriodSeconds: {{ .Values.timeToFinishWork }}
          # never restart a pod because the same pod would reused the volume
          restartPolicy: Never
          # creates temporaty volume on kube worker node (it is deleted automatically as soon as POD is removed from history)
          volumes:
          - name: shell-volume
            emptyDir:
              sizeLimit: {{ index .Values "resources" "requests" "ephemeral-storage" }}
          - name: {{ include "service.name" . }}
            configMap:
              name: {{ include "service.name" . }}
